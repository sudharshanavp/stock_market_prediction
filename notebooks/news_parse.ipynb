{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Parse prototyping\n",
    "\n",
    "for sentiment analysis of a specified stock we need to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, NoReturn\n",
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stopwords = stopwords.words(\"english\")\n",
    "\n",
    "\n",
    "def get_page(url_string: str) -> BeautifulSoup:\n",
    "    \"\"\"Scrape the page specified by the input url\n",
    "\n",
    "    Args:\n",
    "        url_string (str): input url\n",
    "\n",
    "    Returns:\n",
    "        BeautifulSoup: return the page obtained\n",
    "    \"\"\"\n",
    "\n",
    "    response = get(url_string)\n",
    "    return BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "\n",
    "def clean_article(article: str) -> List[str]:\n",
    "    \"\"\"Clean the article by removing punctuations and stop words\n",
    "\n",
    "    Args:\n",
    "        article (str): article content in string format\n",
    "\n",
    "    Returns:\n",
    "        List[str]: list of words in the article\n",
    "    \"\"\"\n",
    "\n",
    "    article = article.replace(\"/(\\n)/gm\", \" \")\n",
    "    article = re.sub(\"[.,!?:;%&$^*@#)/(-\" '`\"—=+]', \" \", article)\n",
    "    article = re.sub(\"[0-9]\", \" \", article)\n",
    "    article = article.replace(\"`|’|”|“\", \"'\")\n",
    "    article = article.replace(\"/(\\\\x)/g\", \"\")\n",
    "    new_stop_words = [\n",
    "        \"said\",\n",
    "        \"also\",\n",
    "        \"per\",\n",
    "        \"cent\",\n",
    "        \"would\",\n",
    "        \"last\",\n",
    "        \"first\",\n",
    "        \"like\",\n",
    "        \"'\",\n",
    "        '\"',\n",
    "        \"'\",\n",
    "        '\"',\n",
    "        \"’\",\n",
    "        \"'s\",\n",
    "        \"“\",\n",
    "        \"”\",\n",
    "    ]\n",
    "    stopwords.extend(new_stop_words)\n",
    "    clean = [word for word in word_tokenize(article) if not word in stopwords]\n",
    "    return clean\n",
    "\n",
    "\n",
    "def lower_case(article: List[str]) -> List[str]:\n",
    "    \"\"\"Convert all characters of the words in the article to lowercase\n",
    "\n",
    "    Args:\n",
    "        article (List[str]): words present in the article\n",
    "\n",
    "    Returns:\n",
    "        List[str]: words in the article converted to lowercase\n",
    "    \"\"\"\n",
    "    lower_case_list = [sentence.lower() for sentence in article]\n",
    "    return lower_case_list\n",
    "\n",
    "\n",
    "def sort_dictionary(dictionary: dict) -> dict:\n",
    "    \"\"\"Sort the input dictionary\n",
    "\n",
    "    Args:\n",
    "        dictionary (dict): input dictionary\n",
    "\n",
    "    Returns:\n",
    "        dict: sorted dictionary\n",
    "    \"\"\"\n",
    "    sorted_dict = dict(sorted(dictionary.items(), key=lambda value: value[1]))\n",
    "    return dict(reversed(list(sorted_dict.items())))\n",
    "\n",
    "\n",
    "def export_as_csv(dictionary: dict) -> NoReturn:\n",
    "    \"\"\"Export dictionary as csv file\n",
    "\n",
    "    Args:\n",
    "        dictionary (dict): input dictionary\n",
    "\n",
    "    Returns:\n",
    "        NoReturn: Export the csv file to data folder in project root directory\n",
    "    \"\"\"\n",
    "    export = {\"word\": list(dictionary.keys()), \"frequency\": list(dictionary.values())}\n",
    "    export_dataframe = pd.DataFrame(export)\n",
    "    export_dataframe.to_csv(\"../data/word_frequency.csv\")\n",
    "\n",
    "\n",
    "def update_frequency(element: str, dictionary: dict) -> dict:\n",
    "    \"\"\"Update frequency of each word in the dictionary\n",
    "\n",
    "    Args:\n",
    "        element (str): word to be updated\n",
    "        dictionary (dict): the dictionary containing words and their frequency count\n",
    "\n",
    "    Returns:\n",
    "        dict: updated dictionary\n",
    "    \"\"\"\n",
    "    if element in dictionary:\n",
    "        dictionary[element] += 1\n",
    "    else:\n",
    "        dictionary.update({element: 1})\n",
    "    return dictionary\n",
    "\n",
    "\n",
    "def get_word_frequency(news_list: list) -> NoReturn:\n",
    "    \"\"\"Find the frequency of words in the article\n",
    "\n",
    "    Args:\n",
    "        news_list (list): list of articles parsed from the webpage\n",
    "\n",
    "    Returns:\n",
    "        NoReturn: export word frequncy as a csv file\n",
    "    \"\"\"\n",
    "    word_frequency = {}\n",
    "    for news in news_list:\n",
    "        news = news.replace(\"/\\r?\\n|\\r/g\", \" \")\n",
    "\n",
    "    for news in news_list:\n",
    "        cleaned_article = clean_article(news)\n",
    "        cleaned_article = lower_case(cleaned_article)\n",
    "\n",
    "        for word in cleaned_article:\n",
    "            if len(word) > 2:\n",
    "                word_frequency = update_frequency(word, word_frequency)\n",
    "\n",
    "    word_frequency_sorted = sort_dictionary(dict(word_frequency))\n",
    "    export_as_csv(word_frequency_sorted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing to parse moneycontrol website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the url string for a particular stock\n",
    "\n",
    "URL = \"https://www.moneycontrol.com/news/tags/\"\n",
    "stock_name = \"Aditya Birla\"\n",
    "resultant_url = URL + stock_name.replace(\" \", \"-\") + \".html\"\n",
    "resultant_url\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the value of number of pages of news a particular stock has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_pages = int(get_page(resultant_url).find_all('a', class_=\"last\")[-1]['data-page'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get links of all pages of news of specific stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_links = []\n",
    "for i in range(1, number_of_pages + 1):\n",
    "    page_links.append(resultant_url + \"/page-\" + str(i) + \"/\")\n",
    "page_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_list = []\n",
    "for link in page_links:\n",
    "    article_list = get_page(link).find('ul', id=\"cagetory\").find_all('a')\n",
    "    for article_block in article_list:\n",
    "        headline_list.append(article_block.text) \n",
    "    headline_list = list(filter(('').__ne__, headline_list))\n",
    "headline_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape the news from page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_links = []\n",
    "for link in page_links:\n",
    "    page = get_page(link)\n",
    "    article_tabs = page.find('div', class_='fleft').find_all('li', class_='clearfix')\n",
    "    article_links = [tab.a.get('href') for tab in article_tabs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eb4fb5dfea142a1d2d7a29cfa1c16e0a3126ddd7419aaecf4949844df6a9cd6f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('finbert')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
