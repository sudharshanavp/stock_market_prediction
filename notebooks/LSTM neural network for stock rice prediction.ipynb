{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "bfab9c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules\n",
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "164876e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the data\n",
    "data = pd.read_csv('dataset/TCS.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1e50c800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolating the date and close price\n",
    "data = data[['Date', 'Close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "56089126",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = data.loc[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d1e011d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4139, 2)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "658ec765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature preprocessing\n",
    "new_data = new_data.drop(['Date'], axis = 1)\n",
    "new_data = new_data.reset_index(drop = True)\n",
    "T = new_data.values\n",
    "T = T.astype('float32')\n",
    "T = np.reshape(T, (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "98418bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-max scaling to get the values in the range [0,1] to optimize convergence speed\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "T = scaler.fit_transform(T)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "473ddc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80-20 split\n",
    "train_size = int(len(T) * 0.80)\n",
    "test_size = int(len(T) - train_size)\n",
    "train, test = T[0:train_size,:], T[train_size:len(T),:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c60bb33a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3311, 1), (828, 1))"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape,test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9c2c3b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for create features from the time series data\n",
    "def create_features(data, window_size):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data) - window_size - 1):\n",
    "        window = data[i:(i + window_size), 0]\n",
    "        X.append(window)\n",
    "        Y.append(data[i + window_size, 0])\n",
    "    return np.array(X), np.array(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "84f91079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roughly one month of trading \n",
    "window_size = 7\n",
    "X_train, Y_train = create_features(train, window_size)\n",
    "\n",
    "X_test, Y_test = create_features(test, window_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "f4c18c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3303, 7), (3303,), (820, 7), (820,))"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,Y_train.shape,X_test.shape,Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c5e298bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/30\n",
      "166/166 [==============================] - 3s 12ms/step - loss: 0.0381 - val_loss: 0.0017\n",
      "Epoch 2/30\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.0032 - val_loss: 0.0017\n",
      "Epoch 3/30\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.0031 - val_loss: 0.0019\n",
      "Epoch 4/30\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.0028 - val_loss: 0.0018\n",
      "Epoch 5/30\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.0028 - val_loss: 0.0017\n",
      "Epoch 6/30\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.0024 - val_loss: 0.0017\n",
      "Epoch 7/30\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.0025 - val_loss: 0.0016\n",
      "Epoch 8/30\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.0024 - val_loss: 0.0017\n",
      "Epoch 9/30\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.0024 - val_loss: 0.0017\n",
      "Epoch 10/30\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.0021 - val_loss: 0.0016\n",
      "Epoch 11/30\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.0020 - val_loss: 0.0015\n",
      "Epoch 12/30\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.0019 - val_loss: 0.0014\n",
      "Epoch 13/30\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.0019 - val_loss: 0.0014\n",
      "Epoch 14/30\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.0017 - val_loss: 0.0013\n",
      "Epoch 15/30\n",
      "166/166 [==============================] - 2s 13ms/step - loss: 0.0016 - val_loss: 0.0013\n",
      "Epoch 16/30\n",
      "166/166 [==============================] - 2s 13ms/step - loss: 0.0016 - val_loss: 0.0013\n",
      "Epoch 17/30\n",
      "166/166 [==============================] - 2s 13ms/step - loss: 0.0015 - val_loss: 0.0012\n",
      "Epoch 18/30\n",
      "166/166 [==============================] - 2s 13ms/step - loss: 0.0015 - val_loss: 0.0012\n",
      "Epoch 19/30\n",
      "166/166 [==============================] - 2s 13ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 20/30\n",
      "166/166 [==============================] - 2s 13ms/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 21/30\n",
      "166/166 [==============================] - 2s 13ms/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 22/30\n",
      "166/166 [==============================] - 2s 13ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 23/30\n",
      "166/166 [==============================] - 2s 13ms/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 24/30\n",
      "166/166 [==============================] - 2s 15ms/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 25/30\n",
      "166/166 [==============================] - 2s 14ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 26/30\n",
      "166/166 [==============================] - 2s 13ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 27/30\n",
      "166/166 [==============================] - 2s 14ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 28/30\n",
      "166/166 [==============================] - 2s 13ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 29/30\n",
      "166/166 [==============================] - 2s 13ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 30/30\n",
      "166/166 [==============================] - 2s 13ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Train RMSE is: \n",
      "46.11371014382253 \n",
      "\n",
      "Test RMSE is: \n",
      "107.89728625411125\n"
     ]
    }
   ],
   "source": [
    "# Reshape to the format of [samples, time steps, features]\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "T_shape = T.shape\n",
    "train_shape = train.shape\n",
    "test_shape = test.shape\n",
    "\n",
    "# Make sure that the number of rows in the dataset = train rows + test rows\n",
    "def isLeak(T_shape, train_shape, test_shape):\n",
    "    return not(T_shape[0] == (train_shape[0] + test_shape[0]))\n",
    "\n",
    "print(isLeak(T_shape, train_shape, test_shape))\n",
    "\n",
    "# Model imports\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "\n",
    "# Setting seed for reproducibility \n",
    "tf.random.set_seed(11)\n",
    "np.random.seed(11)\n",
    "\n",
    "# Building model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(units = 50, activation = 'relu', #return_sequences = True, \n",
    "               input_shape = (X_train.shape[1], window_size)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Optional additional model layer to make a deep network. If you want to use this, uncomment #return_sequences param in previous add\n",
    "\"\"\"\n",
    "model.add(LSTM(units = 25, activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\"\"\"\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1))\n",
    "model.compile(loss = 'mean_squared_error', optimizer = 'adam')\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs = 30, batch_size = 20, validation_data = (X_test, Y_test))\n",
    "\n",
    "\"\"\"\n",
    "Loading the best model and predicting\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Predicting and inverse transforming the predictions\n",
    "\n",
    "train_predict = model.predict(X_train)\n",
    "\n",
    "Y_hat_train = scaler.inverse_transform(train_predict)\n",
    "\n",
    "test_predict = model.predict(X_test)\n",
    "\n",
    "Y_hat_test = scaler.inverse_transform(test_predict)\n",
    "\n",
    "# Inverse transforming the actual values, to return them to their original values\n",
    "Y_test = scaler.inverse_transform([Y_test])\n",
    "Y_train = scaler.inverse_transform([Y_train])\n",
    "\n",
    "# Reshaping \n",
    "Y_hat_train = np.reshape(Y_hat_train, newshape =X_train.shape[0])\n",
    "Y_hat_test = np.reshape(Y_hat_test, newshape = X_test.shape[0])\n",
    "\n",
    "Y_train = np.reshape(Y_train, newshape = X_train.shape[0])\n",
    "Y_test = np.reshape(Y_test, newshape = X_test.shape[0])\n",
    "\n",
    "# Model performance evaluation\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "train_RMSE = np.sqrt(mean_squared_error(Y_train, Y_hat_train))\n",
    "\n",
    "test_RMSE = np.sqrt(mean_squared_error(Y_test, Y_hat_test))\n",
    "\n",
    "print('Train RMSE is: ')\n",
    "print(train_RMSE, '\\n')\n",
    "print('Test RMSE is: ')\n",
    "print(test_RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b80325",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
